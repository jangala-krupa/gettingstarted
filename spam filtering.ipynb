{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "import random\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir = \"/home/user/Desktop/gitrepo/nlpmodels/spamfilter\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('/home/user/Desktop/gitrepo/nlpmodels/spamfilter', ['enron6', 'enron4', 'enron3', 'enron2', 'enron5', 'enron1', '.ipynb_checkpoints'], 8)\n",
      "('/home/user/Desktop/gitrepo/nlpmodels/spamfilter/enron6', ['ham', 'spam'], 1)\n",
      "('/home/user/Desktop/gitrepo/nlpmodels/spamfilter/enron6/ham', [], 1500)\n",
      "('/home/user/Desktop/gitrepo/nlpmodels/spamfilter/enron6/spam', [], 4500)\n",
      "('/home/user/Desktop/gitrepo/nlpmodels/spamfilter/enron4', ['ham', 'spam'], 1)\n",
      "('/home/user/Desktop/gitrepo/nlpmodels/spamfilter/enron4/ham', [], 1500)\n",
      "('/home/user/Desktop/gitrepo/nlpmodels/spamfilter/enron4/spam', [], 4500)\n",
      "('/home/user/Desktop/gitrepo/nlpmodels/spamfilter/enron3', ['ham', 'spam'], 1)\n",
      "('/home/user/Desktop/gitrepo/nlpmodels/spamfilter/enron3/ham', [], 4012)\n",
      "('/home/user/Desktop/gitrepo/nlpmodels/spamfilter/enron3/spam', [], 1500)\n",
      "('/home/user/Desktop/gitrepo/nlpmodels/spamfilter/enron2', ['ham', 'spam'], 1)\n",
      "('/home/user/Desktop/gitrepo/nlpmodels/spamfilter/enron2/ham', [], 4361)\n",
      "('/home/user/Desktop/gitrepo/nlpmodels/spamfilter/enron2/spam', [], 1496)\n",
      "('/home/user/Desktop/gitrepo/nlpmodels/spamfilter/enron5', ['ham', 'spam'], 1)\n",
      "('/home/user/Desktop/gitrepo/nlpmodels/spamfilter/enron5/ham', [], 1500)\n",
      "('/home/user/Desktop/gitrepo/nlpmodels/spamfilter/enron5/spam', [], 3675)\n",
      "('/home/user/Desktop/gitrepo/nlpmodels/spamfilter/enron1', ['ham', 'spam'], 1)\n",
      "('/home/user/Desktop/gitrepo/nlpmodels/spamfilter/enron1/ham', [], 3672)\n",
      "('/home/user/Desktop/gitrepo/nlpmodels/spamfilter/enron1/spam', [], 1500)\n",
      "('/home/user/Desktop/gitrepo/nlpmodels/spamfilter/.ipynb_checkpoints', [], 1)\n"
     ]
    }
   ],
   "source": [
    "#in this i'm just trying to loop around the directories and the subdirectories where my data is present\n",
    "for directories, subdirs, files in os.walk(rootdir):\n",
    "    print(directories, subdirs, len(files))\n",
    "    #print all the files and the subdirectories along with the length of the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('/home/user/Desktop/gitrepo/nlpmodels/spamfilter/enron1', 'ham')\n",
      "/home/user/Desktop/gitrepo/nlpmodels/spamfilter/enron1\n",
      "ham\n"
     ]
    }
   ],
   "source": [
    "print(os.path.split(\"/home/user/Desktop/gitrepo/nlpmodels/spamfilter/enron1/ham\"))\n",
    "print(os.path.split(\"/home/user/Desktop/gitrepo/nlpmodels/spamfilter/enron1/ham\")[0])\n",
    "print(os.path.split(\"/home/user/Desktop/gitrepo/nlpmodels/spamfilter/enron1/ham\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('/home/user/Desktop/gitrepo/nlpmodels/spamfilter/enron6/ham', [], 1500)\n",
      "('/home/user/Desktop/gitrepo/nlpmodels/spamfilter/enron4/ham', [], 1500)\n",
      "('/home/user/Desktop/gitrepo/nlpmodels/spamfilter/enron3/ham', [], 4012)\n",
      "('/home/user/Desktop/gitrepo/nlpmodels/spamfilter/enron2/ham', [], 4361)\n",
      "('/home/user/Desktop/gitrepo/nlpmodels/spamfilter/enron5/ham', [], 1500)\n",
      "('/home/user/Desktop/gitrepo/nlpmodels/spamfilter/enron1/ham', [], 3672)\n",
      "('/home/user/Desktop/gitrepo/nlpmodels/spamfilter/enron6/spam', [], 4500)\n",
      "('/home/user/Desktop/gitrepo/nlpmodels/spamfilter/enron4/spam', [], 4500)\n",
      "('/home/user/Desktop/gitrepo/nlpmodels/spamfilter/enron3/spam', [], 1500)\n",
      "('/home/user/Desktop/gitrepo/nlpmodels/spamfilter/enron2/spam', [], 1496)\n",
      "('/home/user/Desktop/gitrepo/nlpmodels/spamfilter/enron5/spam', [], 3675)\n",
      "('/home/user/Desktop/gitrepo/nlpmodels/spamfilter/enron1/spam', [], 1500)\n"
     ]
    }
   ],
   "source": [
    "for directories, subdirs, files in os.walk(rootdir):\n",
    "    if (os.path.split(directories)[1]  == 'ham'):\n",
    "        print(directories, subdirs, len(files))\n",
    "for directories, subdirs, files in os.walk(rootdir):\n",
    "    if (os.path.split(directories)[1]  == 'spam'):\n",
    "        print(directories, subdirs, len(files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: re : ios duke energy\n",
      "- - - - - - - - - - - - - - - - - - - - - - forwarded by dennis lee / et & s / enron on 08 / 10 / 2000 04 : 02\n",
      "pm - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
      "ted chavez\n",
      "08 / 08 / 2000 02 : 46 pm\n",
      "to : dennis lee / et & s / enron @ enron\n",
      "cc :\n",
      "subject : re : ios duke energy\n",
      "credit is approved .\n",
      "from : dennis lee 08 / 08 / 2000 02 : 14 pm\n",
      "to : ted chavez / gpgfin / enron @ enron\n",
      "cc :\n",
      "subject : ios duke energy\n",
      "credit request\n",
      "shipper : duke energy trading & marketing\n",
      "contract : ios\n",
      "term : 04 / 01 / 01 thru 10 / 31 / 01\n",
      "volume : 14 , 000 dth / d\n",
      "- - - - - - - - - - - - - - - - - - - - - - forwarded by dennis lee / et & s / enron on 08 / 10 / 2000 04 : 02\n",
      "pm - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
      "ted chavez\n",
      "08 / 08 / 2000 02 : 46 pm\n",
      "to : dennis lee / et & s / enron @ enron\n",
      "cc :\n",
      "subject : re : ios sempra energy\n",
      "credit is approved .\n",
      "from : dennis lee 08 / 08 / 2000 02 : 15 pm\n",
      "to : ted chavez / gpgfin / enron @ enron\n",
      "cc :\n",
      "subject : ios sempra energy\n",
      "credit request\n",
      "shipper : sempra energy trading\n",
      "contract : ios\n",
      "term : 04 / 01 / 01 thru 10 / 31 / 01\n",
      "volume : 14 , 000 dth / d\n",
      "- - - - - - - - - - - - - - - - - - - - - - forwarded by dennis lee / et & s / enron on 08 / 10 / 2000 04 : 02\n",
      "pm - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
      "ted chavez\n",
      "08 / 08 / 2000 03 : 23 pm\n",
      "to : dennis lee / et & s / enron @ enron\n",
      "cc :\n",
      "subject : re : ios eprime\n",
      "credit is approved .\n",
      "from : dennis lee 08 / 08 / 2000 02 : 13 pm\n",
      "to : ted chavez / gpgfin / enron @ enron\n",
      "cc :\n",
      "subject : ios eprime\n",
      "credit request\n",
      "shipper : e prime , inc .\n",
      "contract : ios\n",
      "term : 04 / 01 / 01 thru 10 / 31 / 01\n",
      "volume : 14 , 000 dth / d\n",
      "( this replaces the request i sent you this morning )\n",
      "- - - - - - - - - - - - - - - - - - - - - - forwarded by dennis lee / et & s / enron on 08 / 10 / 2000 04 : 02\n",
      "pm - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
      "ted chavez\n",
      "08 / 08 / 2000 04 : 55 pm\n",
      "to : dennis lee / et & s / enron @ enron\n",
      "cc :\n",
      "subject : re : ios texaco natural gas\n",
      "credit is approved .\n",
      "from : dennis lee 08 / 08 / 2000 04 : 24 pm\n",
      "to : ted chavez / gpgfin / enron @ enron\n",
      "cc :\n",
      "subject : ios texaco natural gas\n",
      "credit request\n",
      "shipper : texaco natural gas , inc .\n",
      "contract : ios\n",
      "term : 04 / 01 / 01 thru 10 / 31 / 01\n",
      "volume : 14 , 000 dth / d\n",
      "- - - - - - - - - - - - - - - - - - - - - - forwarded by dennis lee / et & s / enron on 08 / 10 / 2000 04 : 02\n",
      "pm - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
      "ted chavez\n",
      "08 / 08 / 2000 04 : 56 pm\n",
      "to : dennis lee / et & s / enron @ enron\n",
      "cc :\n",
      "subject : re : ios coral energy resources , l . p .\n",
      "credit is approved .\n",
      "from : dennis lee 08 / 08 / 2000 04 : 24 pm\n",
      "to : ted chavez / gpgfin / enron @ enron\n",
      "cc :\n",
      "subject : ios coral energy resources , l . p .\n",
      "credit request\n",
      "shipper : coral energy resources l . p .\n",
      "contract : ios\n",
      "term : 04 / 01 / 01 thru 10 / 31 / 01\n",
      "volume : 14 , 000 dth / d\n",
      "- - - - - - - - - - - - - - - - - - - - - - forwarded by dennis lee / et & s / enron on 08 / 10 / 2000 04 : 02\n",
      "pm - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
      "ted chavez\n",
      "08 / 08 / 2000 04 : 57 pm\n",
      "to : dennis lee / et & s / enron @ enron\n",
      "cc :\n",
      "subject : re : ios western gas resources , inc .\n",
      "credit is approved .\n",
      "from : dennis lee 08 / 08 / 2000 04 : 28 pm\n",
      "to : ted chavez / gpgfin / enron @ enron\n",
      "cc :\n",
      "subject : ios western gas resources , inc .\n",
      "credit request\n",
      "shipper : western gas resources , inc .\n",
      "contract : ios\n",
      "term : 04 / 01 / 01 thru 10 / 31 / 01\n",
      "volume : 14 , 000 dth / d\n",
      "- - - - - - - - - - - - - - - - - - - - - - forwarded by dennis lee / et & s / enron on 08 / 10 / 2000 04 : 02\n",
      "pm - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
      "ted chavez\n",
      "08 / 09 / 2000 12 : 33 pm\n",
      "to : dennis lee / et & s / enron @ enron\n",
      "cc :\n",
      "subject : re : ios oneok energy marketing & trading , l . p .\n",
      "credit is approved .\n",
      "from : dennis lee 08 / 09 / 2000 09 : 03 am\n",
      "to : ted chavez / gpgfin / enron @ enron\n",
      "cc :\n",
      "subject : ios oneok energy marketing & trading , l . p .\n",
      "credit request\n",
      "shipper : oneok energy marketing & trading , l . p .\n",
      "contract : ios\n",
      "term : 04 / 01 / 01 thru 10 / 31 / 01\n",
      "volume : 14 , 000 dth / d\n",
      "- - - - - - - - - - - - - - - - - - - - - - forwarded by dennis lee / et & s / enron on 08 / 10 / 2000 04 : 02\n",
      "pm - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
      "ted chavez\n",
      "08 / 09 / 2000 12 : 34 pm\n",
      "to : dennis lee / et & s / enron @ enron\n",
      "cc :\n",
      "subject : re : ios dynegy marketing & trade\n",
      "credit is approved .\n",
      "from : dennis lee 08 / 09 / 2000 09 : 04 am\n",
      "to : ted chavez / gpgfin / enron @ enron\n",
      "cc :\n",
      "subject : ios dynegy marketing & trade\n",
      "credit request\n",
      "shipper : dynegy marketing & trade\n",
      "contract : ios\n",
      "term : 04 / 01 / 01 thru 10 / 31 / 01\n",
      "volume : 14 , 000 dth / d\n",
      "- - - - - - - - - - - - - - - - - - - - - - forwarded by dennis lee / et & s / enron on 08 / 10 / 2000 04 : 02\n",
      "pm - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
      "ted chavez\n",
      "08 / 09 / 2000 12 : 35 pm\n",
      "to : dennis lee / et & s / enron @ enron\n",
      "cc :\n",
      "subject : re : ios questar energy trading\n",
      "credit is approved .\n",
      "from : dennis lee 08 / 09 / 2000 09 : 05 am\n",
      "to : ted chavez / gpgfin / enron @ enron\n",
      "cc :\n",
      "subject : ios questar energy trading\n",
      "credit request\n",
      "shipper : questar energy trading company\n",
      "contract : ios\n",
      "term : 04 / 01 / 01 thru 10 / 31 / 01\n",
      "volume : 14 , 000 dth / d\n",
      "- - - - - - - - - - - - - - - - - - - - - - forwarded by dennis lee / et & s / enron on 08 / 10 / 2000 04 : 02\n",
      "pm - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
      "ted chavez\n",
      "08 / 09 / 2000 12 : 36 pm\n",
      "to : dennis lee / et & s / enron @ enron\n",
      "cc :\n",
      "subject : re : ios reliant energy services\n",
      "credit is approved .\n",
      "from : dennis lee 08 / 09 / 2000 10 : 31 am\n",
      "to : ted chavez / gpgfin / enron @ enron\n",
      "cc :\n",
      "subject : ios reliant energy services\n",
      "credit request\n",
      "shipper : reliant energy services\n",
      "contract : ios\n",
      "term : 04 / 01 / 01 thru 10 / 31 / 01\n",
      "volume : 14 , 000 dth / d\n",
      "Subject: re [ 4 ] : talk with his pills\n",
      "spu\n",
      "m\n",
      "r -\n",
      "th\n",
      "ewe\n",
      "an\n",
      "saf\n",
      "twa\n",
      "ph\n",
      "macy\n",
      "en\n",
      "st\n",
      "dthe\n",
      "es\n",
      "yof\n",
      "ar\n",
      "inc\n",
      "eyo\n",
      "xualdes\n",
      "spe\n",
      "umeby\n",
      "%\n",
      "reas\n",
      "urse\n",
      "ireand\n",
      "rmvol\n",
      "500\n",
      "100\n",
      "uraland\n",
      "deeff\n",
      "- incon\n",
      "ttowel\n",
      "wnbra\n",
      "% nat\n",
      "nosi\n",
      "ects\n",
      "tras\n",
      "l - kno\n",
      "nds .\n",
      "expe\n",
      "cethr\n",
      "eslon\n",
      "gas\n",
      "rien\n",
      "eetim\n",
      "geror\n",
      "ms\n",
      "wor\n",
      "deshi\n",
      "gwit\n",
      "hou\n",
      "ldwi\n",
      "ppin\n",
      "hin 24\n",
      "rs\n",
      "arm inc e yo xual des spe\n",
      "ume by % reas ur se ire and\n",
      "rm vol 500 100 ural and de eff\n",
      "- in con t to wel wn bra % nat no si\n",
      "ects tras l - kno nds . expe\n",
      "ce thr es lon gas rien ee tim\n",
      "ger or ms wor de shi g wit\n",
      "hou ld wi ppin hin 24 rs\n",
      "sp - m ur the we\n",
      "and saf wa ph acy\n",
      "is ne st the est\n",
      "y of arm inc e yo xual des\n",
      "spe ume by % reas ur se\n",
      "ire and rm vol 500 100 ural and\n",
      "de eff - in con t to wel wn bra % nat\n",
      "no si ects tras l - kno nds .\n",
      "expe ce thr es lon gas rien\n",
      "ee tim ger or ms wor de shi\n",
      "g wit hou ld wi ppin hin 24\n",
      "rs sp - m ur the\n",
      "we and saf wa ph\n",
      "acy is ne st the\n",
      "est y of arm inc e yo\n",
      "xual des spe ume by % reas\n",
      "ur se ire and rm vol 500 100\n",
      "ural and de eff - in con t to wel wn bra\n",
      "% nat no si ects tras l - kno\n",
      "nds . expe ce thr es lon gas\n",
      "rien ee tim ger or ms wor\n",
      "de shi g wit hou ld wi ppin\n",
      "hin 24 rs sp - m ur\n",
      "the we and saf wa\n",
      "ph acy is ne st\n",
      "the est y of arm inc\n",
      "e yo xual des spe ume by %\n",
      "reas ur se ire and rm vol 500\n",
      "100 ural and de eff - in con t to wel\n",
      "wn bra % nat no si ects tras\n",
      "l - kno nds . expe ce thr es lon\n",
      "gas rien ee tim ger or ms\n",
      "wor de shi g wit hou ld wi\n",
      "ppin hin 24 rs sp - m\n",
      "ur the we and saf\n",
      "wa ph acy is ne\n",
      "st the est y of arm\n",
      "inc e yo xual des spe ume by\n",
      "% reas ur se ire and rm vol\n",
      "500 100 ural and de eff - in con\n",
      "t to wel wn bra % nat no si ects\n",
      "tras l - kno nds . expe ce thr\n",
      "es lon gas rien ee tim ger or\n",
      "ms wor de shi g wit hou\n",
      "ld wi ppin hin 24 rs sp\n",
      "- m ur the we and\n",
      "saf wa ph acy is\n",
      "ne st the est y of\n",
      "arm inc e yo xual des spe\n",
      "ume by % reas ur se ire and\n",
      "rm vol 500 100 ural and de eff\n",
      "- in con t to wel wn bra % nat no si\n",
      "ects tras l - kno nds . expe\n",
      "ce thr es lon gas rien ee tim\n",
      "ger or ms wor de shi g wit\n",
      "hou ld wi ppin hin 24 rs\n",
      "sp - m ur the we\n",
      "and saf wa ph acy\n",
      "is ne st the est\n",
      "y of arm inc e yo xual des\n",
      "spe ume by % reas ur se\n",
      "ire and rm vol 500 100 ural and\n",
      "de eff - in con t to wel wn bra % nat\n",
      "no si ects tras l - kno nds .\n",
      "expe ce thr es lon gas rien\n",
      "ee tim ger or ms wor de shi\n",
      "g wit hou ld wi ppin hin 24\n",
      "rs sp - m ur the\n",
      "we and saf wa ph\n",
      "acy is ne st the\n",
      "est y of arm inc e yo\n",
      "xual des spe ume by % reas\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ham_list = []\n",
    "spam_list = []\n",
    "\n",
    "# Same as before, but this time, read the files, and append them to the ham and spam list\n",
    "for directories, subdirs, files in os.walk(rootdir):\n",
    "    if (os.path.split(directories)[1]  == 'ham'):\n",
    "        for filename in files:      \n",
    "            with io.open(os.path.join(directories, filename),encoding=\"latin-1\") as f:\n",
    "                data = f.read()\n",
    "                ham_list.append(data)\n",
    "    \n",
    "    if (os.path.split(directories)[1]  == 'spam'):\n",
    "        for filename in files:\n",
    "            with io.open(os.path.join(directories, filename),encoding=\"latin-1\") as f:\n",
    "                data = f.read()\n",
    "                spam_list.append(data)\n",
    "\n",
    "\n",
    "print(ham_list[0])\n",
    "print(spam_list[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': True, 'brown': True, 'fox': True, 'quick': True, 'the': True}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_word_features(words):\n",
    "    my_dict = dict( [ (word, True) for word in words] )\n",
    "    return my_dict\n",
    "\n",
    "create_word_features([\"the\", \"quick\", \"brown\", \"quick\", \"a\", \"fox\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({u'reliant': True, u'trade': True, u'24': True, u're': True, u'(': True, u'28': True, u',': True, u'to': True, u'sent': True, u'@': True, u'dth': True, u'prime': True, u'term': True, u'd': True, u'55': True, u'57': True, u'56': True, u'l': True, u'morning': True, u'p': True, u'Subject': True, u'energy': True, u'ted': True, u'dynegy': True, u'trading': True, u'et': True, u'subject': True, u'02': True, u'03': True, u'01': True, u'04': True, u'05': True, u'08': True, u'09': True, u'/': True, u'gpgfin': True, u'lee': True, u'marketing': True, u'by': True, u'enron': True, u'eprime': True, u'on': True, u'shipper': True, u'33': True, u'coral': True, u'23': True, u'36': True, u'contract': True, u'34': True, u'credit': True, u's': True, u'duke': True, u'cc': True, u'questar': True, u'from': True, u'forwarded': True, u'&': True, u'texaco': True, u'.': True, u'thru': True, u':': True, u'pm': True, u'company': True, u'gas': True, u'volume': True, u'western': True, u'e': True, u'10': True, u'13': True, u'replaces': True, u'15': True, u'14': True, u'this': True, u'resources': True, u'31': True, u'is': True, u'am': True, u'35': True, u'dennis': True, u'inc': True, u')': True, u'-': True, u'you': True, u'chavez': True, u'ios': True, u'oneok': True, u'000': True, u'services': True, u'approved': True, u'natural': True, u'i': True, u'request': True, u'sempra': True, u'46': True, u'2000': True, u'the': True, u'12': True}, 'ham')\n",
      "({u'ects': True, u'xual': True, u'ireand': True, u'24': True, u'ld': True, u'wel': True, u'to': True, u'ttowel': True, u'4': True, u'th': True, u'uraland': True, u'dthe': True, u'his': True, u'yo': True, u'rmvol': True, u'de': True, u'hin': True, u'reas': True, u'ldwi': True, u'l': True, u'ire': True, u't': True, u'tras': True, u'nosi': True, u'Subject': True, u'en': True, u'ee': True, u'umeby': True, u'deshi': True, u'arm': True, u'es': True, u'rien': True, u'est': True, u'rs': True, u'wor': True, u'ume': True, u're': True, u'rm': True, u'urse': True, u'we': True, u'wa': True, u'wn': True, u'wi': True, u'shi': True, u'100': True, u'by': True, u'g': True, u'of': True, u'or': True, u'nds': True, u'gwit': True, u'eslon': True, u'con': True, u'ce': True, u'wit': True, u'xualdes': True, u'[': True, u'bra': True, u'twa': True, u'lon': True, u'eetim': True, u'eyo': True, u'.': True, u'ppin': True, u'ph': True, u':': True, u'500': True, u'gas': True, u'spu': True, u'ural': True, u'with': True, u'des': True, u'ur': True, u'r': True, u'ms': True, u'expe': True, u'and': True, u'ger': True, u'vol': True, u'is': True, u'saf': True, u'an': True, u'ar': True, u'in': True, u'incon': True, u'spe': True, u'inc': True, u'%': True, u'no': True, u'yof': True, u'-': True, u'ne': True, u'ewe': True, u'pills': True, u'tim': True, u'hou': True, u'nat': True, u'deeff': True, u'eff': True, u'cethr': True, u'geror': True, u'macy': True, u'wnbra': True, u']': True, u'acy': True, u'kno': True, u'e': True, u'sp': True, u'm': True, u'thr': True, u'st': True, u'si': True, u'talk': True, u'y': True, u'the': True, u'se': True}, 'spam')\n"
     ]
    }
   ],
   "source": [
    "ham_list = []\n",
    "spam_list = []\n",
    "\n",
    "# Same as before, but this time:\n",
    "\n",
    "# 1. Break the sentences into words using word_tokenize\n",
    "# 2. Use the create_word_features() function you just wrote\n",
    "for directories, subdirs, files in os.walk(rootdir):\n",
    "    if (os.path.split(directories)[1]  == 'ham'):\n",
    "        for filename in files:      \n",
    "            with io.open(os.path.join(directories, filename), encoding=\"latin-1\") as f:\n",
    "                data = f.read()\n",
    "                \n",
    "                # The data we read is one big string. We need to break it into words.\n",
    "                words = word_tokenize(data)\n",
    "                \n",
    "                ham_list.append((create_word_features(words), \"ham\"))\n",
    "    \n",
    "    if (os.path.split(directories)[1]  == 'spam'):\n",
    "        for filename in files:\n",
    "            with io.open(os.path.join(directories, filename), encoding=\"latin-1\") as f:\n",
    "                data = f.read()\n",
    "                \n",
    "                # The data we read is one big string. We need to break it into words.\n",
    "                words = word_tokenize(data)\n",
    "                \n",
    "                spam_list.append((create_word_features(words), \"spam\"))\n",
    "print(ham_list[0])\n",
    "print(spam_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33716\n"
     ]
    }
   ],
   "source": [
    "combined_list = ham_list + spam_list\n",
    "print(len(combined_list))\n",
    "\n",
    "random.shuffle(combined_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33716\n",
      "23601\n",
      "10115\n"
     ]
    }
   ],
   "source": [
    "# Create a test and train section.\n",
    "\n",
    "# 70% of the data is training. 30% is test\n",
    "\n",
    "training_part = int(len(combined_list) * .7)\n",
    "\n",
    "print(len(combined_list))\n",
    "\n",
    "training_set = combined_list[:training_part]\n",
    "\n",
    "test_set =  combined_list[training_part:]\n",
    "\n",
    "print (len(training_set))\n",
    "print (len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy is: ', 98.49728126544736)\n"
     ]
    }
   ],
   "source": [
    "# Create the Naive Bayes filter\n",
    "\n",
    "classifier = NaiveBayesClassifier.train(training_set)\n",
    "\n",
    "# Find the accuracy, using the test data\n",
    "\n",
    "accuracy = nltk.classify.util.accuracy(classifier, test_set)\n",
    "\n",
    "print(\"Accuracy is: \", accuracy * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                     hpl = True              ham : spam   =    357.2 : 1.0\n",
      "              scheduling = True              ham : spam   =    307.9 : 1.0\n",
      "                     ect = True              ham : spam   =    284.6 : 1.0\n",
      "                     xls = True              ham : spam   =    281.1 : 1.0\n",
      "                    meds = True             spam : ham    =    280.3 : 1.0\n",
      "                 stinson = True              ham : spam   =    274.2 : 1.0\n",
      "                crenshaw = True              ham : spam   =    263.0 : 1.0\n",
      "             medications = True             spam : ham    =    211.5 : 1.0\n",
      "                 parsing = True              ham : spam   =    207.6 : 1.0\n",
      "                  louise = True              ham : spam   =    203.8 : 1.0\n",
      "              macromedia = True             spam : ham    =    198.7 : 1.0\n",
      "                     713 = True              ham : spam   =    198.0 : 1.0\n",
      "                       \u0001 = True              ham : spam   =    188.5 : 1.0\n",
      "               schedules = True              ham : spam   =    186.2 : 1.0\n",
      "                   daren = True              ham : spam   =    171.3 : 1.0\n",
      "                   drugs = True             spam : ham    =    163.6 : 1.0\n",
      "                     sex = True             spam : ham    =    139.6 : 1.0\n",
      "        responsibilities = True              ham : spam   =    133.4 : 1.0\n",
      "                    dose = True             spam : ham    =    128.5 : 1.0\n",
      "             derivatives = True              ham : spam   =    126.5 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter the number of messages you wanted to check2\n",
      "enter the messageSubject: tw ios attached please find the ios procedures which were posted on the internet ( thank you al gore ! ) today for next week ' s ios .\n",
      "(' message is :', 'ham')\n",
      "enter the message \"To start off, I have a 6 new videos + transcripts in the members section. In it, we analyse the Enron email dataset, half a million files, spread over 2.5GB. It's about 1.5 hours of  video.I have also created a Conda environment for running the code (both free and member lessons). This is to ensure everyone is running the same version of libraries, preventing the Works on my machine problems. If you get a second, do you mind trying it here?\"\n",
      "(' message is :', 'ham')\n"
     ]
    }
   ],
   "source": [
    "n = raw_input(\"enter the number of messages you wanted to check\")\n",
    "val=int(n)\n",
    "for i in range(0,val):\n",
    "    msg=raw_input(\"enter the message\")\n",
    "    words=word_tokenize(msg)\n",
    "    features=create_word_features(words)\n",
    "    print(\" message is :\",classifier.classify(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
